{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# nlp100 chapter9\n",
    "\n",
    "Try:http://www.cl.ecei.tohoku.ac.jp/nlp100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 80. コーパスの整形"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "words = []\n",
    "result = []\n",
    "\n",
    "n = 0\n",
    "with open('data/80.txt', 'w') as writer:\n",
    "    with open('data/enwiki-20150112-400-r100-10576.txt') as reader:\n",
    "        for line in reader:\n",
    "            words = re.compile(r'[\\s]').split(line)\n",
    "            for word in words:\n",
    "                word = re.sub(r'^[\\.,!\\?;:()\\[\\]\\'\"]*', r'', word)\n",
    "                word = re.sub(r'[\\.,!\\?;:()\\[\\]\\'\"]*$', r'', word)\n",
    "                if word:\n",
    "                    result.append(word)\n",
    "            if result:\n",
    "                writer.writelines(\" \".join(result) + \"\\n\") \n",
    "                result = [] \n",
    "            n += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 81. 複合語からなる国名への対処"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "American Samoa / American_Samoa\n['sed', '-i', '-e', 's/American Samoa/American_Samoa/g', 'data/80.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antigua and Barbuda / Antigua_and_Barbuda\n['sed', '-i', '-e', 's/Antigua and Barbuda/Antigua_and_Barbuda/g', 'data/80.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ashmore and Cartier Islands / Ashmore_and_Cartier_Islands\n['sed', '-i', '-e', 's/Ashmore and Cartier Islands/Ashmore_and_Cartier_Islands/g', 'data/80.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Bahamas  / The_Bahamas_\n['sed', '-i', '-e', 's/The Bahamas /The_Bahamas_/g', 'data/80.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bassas da India / Bassas_da_India\n['sed', '-i', '-e', 's/Bassas da India/Bassas_da_India/g', 'data/80.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bosnia and Herzegovina / Bosnia_and_Herzegovina\n['sed', '-i', '-e', 's/Bosnia and Herzegovina/Bosnia_and_Herzegovina/g', 'data/80.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bouvet Island / Bouvet_Island\n['sed', '-i', '-e', 's/Bouvet Island/Bouvet_Island/g', 'data/80.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "British Indian Ocean Territory / British_Indian_Ocean_Territory\n['sed', '-i', '-e', 's/British Indian Ocean Territory/British_Indian_Ocean_Territory/g', 'data/80.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "British Virgin Islands / British_Virgin_Islands\n['sed', '-i', '-e', 's/British Virgin Islands/British_Virgin_Islands/g', 'data/80.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Burkina Faso / Burkina_Faso\n['sed', '-i', '-e', 's/Burkina Faso/Burkina_Faso/g', 'data/80.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cape Verde / Cape_Verde\n['sed', '-i', '-e', 's/Cape Verde/Cape_Verde/g', 'data/80.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cayman Islands / Cayman_Islands\n['sed', '-i', '-e', 's/Cayman Islands/Cayman_Islands/g', 'data/80.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Central African Republic / Central_African_Republic\n['sed', '-i', '-e', 's/Central African Republic/Central_African_Republic/g', 'data/80.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Christmas Island / Christmas_Island\n['sed', '-i', '-e', 's/Christmas Island/Christmas_Island/g', 'data/80.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clipperton Island / Clipperton_Island\n['sed', '-i', '-e', 's/Clipperton Island/Clipperton_Island/g', 'data/80.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cocos Islands / Cocos_Islands\n['sed', '-i', '-e', 's/Cocos Islands/Cocos_Islands/g', 'data/80.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keeling Islands / Keeling_Islands\n['sed', '-i', '-e', 's/Keeling Islands/Keeling_Islands/g', 'data/80.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cocos Keeling Islands / Cocos_Keeling_Islands\n['sed', '-i', '-e', 's/Cocos Keeling Islands/Cocos_Keeling_Islands/g', 'data/80.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Democratic Republic of the Congo / Democratic_Republic_of_the_Congo\n['sed', '-i', '-e', 's/Democratic Republic of the Congo/Democratic_Republic_of_the_Congo/g', 'data/80.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Republic of the Congo / Republic_of_the_Congo\n['sed', '-i', '-e', 's/Republic of the Congo/Republic_of_the_Congo/g', 'data/80.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cook Islands / Cook_Islands\n['sed', '-i', '-e', 's/Cook Islands/Cook_Islands/g', 'data/80.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coral Sea Islands / Coral_Sea_Islands\n['sed', '-i', '-e', 's/Coral Sea Islands/Coral_Sea_Islands/g', 'data/80.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Costa Rica / Costa_Rica\n['sed', '-i', '-e', 's/Costa Rica/Costa_Rica/g', 'data/80.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cote d'Ivoire / Cote_d'Ivoire\n['sed', '-i', '-e', \"s/Cote d'Ivoire/Cote_d'Ivoire/g\", 'data/80.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Czech Republic / Czech_Republic\n['sed', '-i', '-e', 's/Czech Republic/Czech_Republic/g', 'data/80.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dominican Republic / Dominican_Republic\n['sed', '-i', '-e', 's/Dominican Republic/Dominican_Republic/g', 'data/80.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El Salvador / El_Salvador\n['sed', '-i', '-e', 's/El Salvador/El_Salvador/g', 'data/80.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equatorial Guinea / Equatorial_Guinea\n['sed', '-i', '-e', 's/Equatorial Guinea/Equatorial_Guinea/g', 'data/80.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Europa Island / Europa_Island\n['sed', '-i', '-e', 's/Europa Island/Europa_Island/g', 'data/80.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Falkland Islands / Falkland_Islands\n['sed', '-i', '-e', 's/Falkland Islands/Falkland_Islands/g', 'data/80.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Islas Malvinas / Islas_Malvinas\n['sed', '-i', '-e', 's/Islas Malvinas/Islas_Malvinas/g', 'data/80.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faroe Islands / Faroe_Islands\n['sed', '-i', '-e', 's/Faroe Islands/Faroe_Islands/g', 'data/80.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French Guiana / French_Guiana\n['sed', '-i', '-e', 's/French Guiana/French_Guiana/g', 'data/80.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French Polynesia / French_Polynesia\n['sed', '-i', '-e', 's/French Polynesia/French_Polynesia/g', 'data/80.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French Southern and Antarctic Lands / French_Southern_and_Antarctic_Lands\n['sed', '-i', '-e', 's/French Southern and Antarctic Lands/French_Southern_and_Antarctic_Lands/g', 'data/80.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Gambia  / The_Gambia_\n['sed', '-i', '-e', 's/The Gambia /The_Gambia_/g', 'data/80.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaza Strip / Gaza_Strip\n['sed', '-i', '-e', 's/Gaza Strip/Gaza_Strip/g', 'data/80.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glorioso Islands / Glorioso_Islands\n['sed', '-i', '-e', 's/Glorioso Islands/Glorioso_Islands/g', 'data/80.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guinea Bissau / Guinea_Bissau\n['sed', '-i', '-e', 's/Guinea Bissau/Guinea_Bissau/g', 'data/80.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heard Island and McDonald Islands / Heard_Island_and_McDonald_Islands\n['sed', '-i', '-e', 's/Heard Island and McDonald Islands/Heard_Island_and_McDonald_Islands/g', 'data/80.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Holy See / Holy_See\n['sed', '-i', '-e', 's/Holy See/Holy_See/g', 'data/80.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vatican City / Vatican_City\n['sed', '-i', '-e', 's/Vatican City/Vatican_City/g', 'data/80.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hong Kong / Hong_Kong\n['sed', '-i', '-e', 's/Hong Kong/Hong_Kong/g', 'data/80.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Isle of Man / Isle_of_Man\n['sed', '-i', '-e', 's/Isle of Man/Isle_of_Man/g', 'data/80.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jan Mayen / Jan_Mayen\n['sed', '-i', '-e', 's/Jan Mayen/Jan_Mayen/g', 'data/80.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Juan de Nova Island / Juan_de_Nova_Island\n['sed', '-i', '-e', 's/Juan de Nova Island/Juan_de_Nova_Island/g', 'data/80.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "North Korea  / North_Korea_\n['sed', '-i', '-e', 's/North Korea /North_Korea_/g', 'data/80.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "South Korea  / South_Korea_\n['sed', '-i', '-e', 's/South Korea /South_Korea_/g', 'data/80.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marshall Islands / Marshall_Islands\n['sed', '-i', '-e', 's/Marshall Islands/Marshall_Islands/g', 'data/80.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Federated States of Micronesia  / Federated_States_of_Micronesia_\n['sed', '-i', '-e', 's/Federated States of Micronesia /Federated_States_of_Micronesia_/g', 'data/80.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Navassa Island / Navassa_Island\n['sed', '-i', '-e', 's/Navassa Island/Navassa_Island/g', 'data/80.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Netherlands Antilles / Netherlands_Antilles\n['sed', '-i', '-e', 's/Netherlands Antilles/Netherlands_Antilles/g', 'data/80.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Caledonia / New_Caledonia\n['sed', '-i', '-e', 's/New Caledonia/New_Caledonia/g', 'data/80.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Zealand / New_Zealand\n['sed', '-i', '-e', 's/New Zealand/New_Zealand/g', 'data/80.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norfolk Island / Norfolk_Island\n['sed', '-i', '-e', 's/Norfolk Island/Norfolk_Island/g', 'data/80.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Northern Mariana Islands / Northern_Mariana_Islands\n['sed', '-i', '-e', 's/Northern Mariana Islands/Northern_Mariana_Islands/g', 'data/80.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Papua New Guinea / Papua_New_Guinea\n['sed', '-i', '-e', 's/Papua New Guinea/Papua_New_Guinea/g', 'data/80.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paracel Islands / Paracel_Islands\n['sed', '-i', '-e', 's/Paracel Islands/Paracel_Islands/g', 'data/80.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pitcairn Islands / Pitcairn_Islands\n['sed', '-i', '-e', 's/Pitcairn Islands/Pitcairn_Islands/g', 'data/80.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Puerto Rico / Puerto_Rico\n['sed', '-i', '-e', 's/Puerto Rico/Puerto_Rico/g', 'data/80.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saint Helena / Saint_Helena\n['sed', '-i', '-e', 's/Saint Helena/Saint_Helena/g', 'data/80.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saint Kitts and Nevis / Saint_Kitts_and_Nevis\n['sed', '-i', '-e', 's/Saint Kitts and Nevis/Saint_Kitts_and_Nevis/g', 'data/80.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saint Lucia / Saint_Lucia\n['sed', '-i', '-e', 's/Saint Lucia/Saint_Lucia/g', 'data/80.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saint Pierre and Miquelon / Saint_Pierre_and_Miquelon\n['sed', '-i', '-e', 's/Saint Pierre and Miquelon/Saint_Pierre_and_Miquelon/g', 'data/80.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saint Vincent and the Grenadines / Saint_Vincent_and_the_Grenadines\n['sed', '-i', '-e', 's/Saint Vincent and the Grenadines/Saint_Vincent_and_the_Grenadines/g', 'data/80.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "San Marino / San_Marino\n['sed', '-i', '-e', 's/San Marino/San_Marino/g', 'data/80.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sao Tome and Principe / Sao_Tome_and_Principe\n['sed', '-i', '-e', 's/Sao Tome and Principe/Sao_Tome_and_Principe/g', 'data/80.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saudi Arabia / Saudi_Arabia\n['sed', '-i', '-e', 's/Saudi Arabia/Saudi_Arabia/g', 'data/80.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serbia and Montenegro / Serbia_and_Montenegro\n['sed', '-i', '-e', 's/Serbia and Montenegro/Serbia_and_Montenegro/g', 'data/80.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sierra Leone / Sierra_Leone\n['sed', '-i', '-e', 's/Sierra Leone/Sierra_Leone/g', 'data/80.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solomon Islands / Solomon_Islands\n['sed', '-i', '-e', 's/Solomon Islands/Solomon_Islands/g', 'data/80.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "South Africa / South_Africa\n['sed', '-i', '-e', 's/South Africa/South_Africa/g', 'data/80.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "South Georgia and the South Sandwich Islands / South_Georgia_and_the_South_Sandwich_Islands\n['sed', '-i', '-e', 's/South Georgia and the South Sandwich Islands/South_Georgia_and_the_South_Sandwich_Islands/g', 'data/80.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spratly Islands / Spratly_Islands\n['sed', '-i', '-e', 's/Spratly Islands/Spratly_Islands/g', 'data/80.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sri Lanka / Sri_Lanka\n['sed', '-i', '-e', 's/Sri Lanka/Sri_Lanka/g', 'data/80.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timor Leste / Timor_Leste\n['sed', '-i', '-e', 's/Timor Leste/Timor_Leste/g', 'data/80.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trinidad and Tobago / Trinidad_and_Tobago\n['sed', '-i', '-e', 's/Trinidad and Tobago/Trinidad_and_Tobago/g', 'data/80.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tromelin Island / Tromelin_Island\n['sed', '-i', '-e', 's/Tromelin Island/Tromelin_Island/g', 'data/80.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turks and Caicos Islands / Turks_and_Caicos_Islands\n['sed', '-i', '-e', 's/Turks and Caicos Islands/Turks_and_Caicos_Islands/g', 'data/80.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "United Arab Emirates / United_Arab_Emirates\n['sed', '-i', '-e', 's/United Arab Emirates/United_Arab_Emirates/g', 'data/80.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "United Kingdom / United_Kingdom\n['sed', '-i', '-e', 's/United Kingdom/United_Kingdom/g', 'data/80.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "United States / United_States\n['sed', '-i', '-e', 's/United States/United_States/g', 'data/80.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Virgin Islands / Virgin_Islands\n['sed', '-i', '-e', 's/Virgin Islands/Virgin_Islands/g', 'data/80.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wake Island / Wake_Island\n['sed', '-i', '-e', 's/Wake Island/Wake_Island/g', 'data/80.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wallis and Futuna / Wallis_and_Futuna\n['sed', '-i', '-e', 's/Wallis and Futuna/Wallis_and_Futuna/g', 'data/80.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "West Bank / West_Bank\n['sed', '-i', '-e', 's/West Bank/West_Bank/g', 'data/80.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Western Sahara / Western_Sahara\n['sed', '-i', '-e', 's/Western Sahara/Western_Sahara/g', 'data/80.txt']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import subprocess\n",
    "\n",
    "\n",
    "def changecountryname(beforec, afterc):\n",
    "    string = \"s/\" + beforec + \"/\" + afterc + \"/g\"\n",
    "    cmd = ['sed','-i','', string ,'data/80.txt']\n",
    "    subprocess.call(cmd)\n",
    "\n",
    "lcountry = []\n",
    "with open('data/81c.txt', 'r') as reader:\n",
    "    for line in reader:\n",
    "        country = re.split(r' ',line[:-1])\n",
    "        if len(country) > 1:\n",
    "            lcountry.append(len(country))\n",
    "            print(\" \".join(country),\"/\",\"_\".join(country))\n",
    "            changecountryname(\" \".join(country),\"_\".join(country))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def context_word(words, writer):\n",
    "    listn = 0\n",
    "    for word in words:\n",
    "        cws = random.randint(1, 5)\n",
    "        cwslist = list(range( -1 * cws, cws + 1))\n",
    "        cwslist.remove(0)\n",
    "        if word:\n",
    "            for x in cwslist:\n",
    "                if listn+x >= 0:\n",
    "                    if listn+x < (len(words) - 1):\n",
    "                        writer.writelines(word+\"\\t\"+words[listn+x]+\"\\n\") \n",
    "        listn += 1\n",
    "        \n",
    "with open('data/82.txt', 'w') as writer:\n",
    "    with open('data/80.txt') as reader:\n",
    "        for line in reader:\n",
    "            words = re.compile(r'[\\s]').split(line)\n",
    "            context_word(words, writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 83. 単語／文脈の頻度の計測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist = []\n",
    "lowercase = [chr(i) for i in range(97, 97+26)]\n",
    "uppercase = [chr(i) for i in range(65, 65+26)]\n",
    "wcase = lowercase + uppercase\n",
    "\n",
    "n = 0\n",
    "with open('data/82.txt') as reader:\n",
    "    for line in reader:\n",
    "        wordlist = re.split(\"\\t\", line[:-1])\n",
    "        if wordlist[1] == \"\":\n",
    "            continue\n",
    "        if wordlist[0][0] in wcase and wordlist[1][0] in wcase:\n",
    "            filename = \"data/83/\" + wordlist[0][0] + wordlist[1][0] + \".txt\"\n",
    "            with open(filename, 'a') as writer:\n",
    "                writer.writelines(line)\n",
    "                n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat data/83/Aa.txt | sort -k 1,2 |uniq -c|sort -nr > data/83/aalc.txt\n",
    "!sed -i '' 's/^\\s*//g' data/83/aalc.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cut -f 1 data/83/Aa.txt | sort -k 1 | uniq -c | sort -nr > data/83/aatc.txt\n",
    "!sed -i '' 's/^\\s*//g' data/83/aatc.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cut -f 2 data/83/Aa.txt | sort -k 1 |uniq -c|sort -nr > data/83/aacc.txt\n",
    "!sed -i '' 's/^\\s*//g' data/83/aacc.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 84. 単語文脈行列の作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  938742 data/83/Aa.txt\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l data/83/Aa.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import collections\n",
    "import math\n",
    "\n",
    "wordlist = []\n",
    "tdic = collections.defaultdict(int)\n",
    "cdic = collections.defaultdict(int)\n",
    "\n",
    "N = 938742\n",
    "with open('data/83/aalc10.txt', 'w') as writer:\n",
    "    with open('data/83/aalc.txt') as reader:\n",
    "        for line in reader:\n",
    "            token = line.lstrip().split(' ')\n",
    "            if int(token[0]) > 9:\n",
    "                writer.write(line)\n",
    "\n",
    "with open('data/83/aatc.txt') as reader:\n",
    "    for line in reader:\n",
    "        line = line.lstrip()\n",
    "        string = re.split(\" \",line[:-1])\n",
    "        tdic[string[1]] = int(string[0])\n",
    "        \n",
    "with open('data/83/aacc.txt') as reader:\n",
    "    for line in reader:\n",
    "        line = line.lstrip()\n",
    "        string =re.split(\" \",line[:-1])  \n",
    "        cdic[string[1]]=int(string[0])\n",
    "\n",
    "with open('data/aa_ppmi_matrix.tsv', 'w') as writer:\n",
    "    with open('data/83/aalc10.txt') as reader:\n",
    "        for line in reader:\n",
    "            line = line.lstrip()\n",
    "            string = re.split(\" \", line[:-1])  \n",
    "            wordlist = re.split('\\t', string[1])   \n",
    "            if int(string[0]) > 0 and tdic[wordlist[0]] > 0 and cdic[wordlist[1]] > 0: \n",
    "                wtc = math.log(N) + math.log(int(string[0])) - math.log(tdic[wordlist[0]]) - math.log(cdic[wordlist[1]])\n",
    "            if wtc > 0:\n",
    "                writer.write(string[1]+'\\t'+str(wtc)+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 85. 主成分分析による次元圧縮"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat data/83/*.txt > data/azAZ.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat data/azAZ.txt | sort -k 1,2 |uniq -c|sort -nr > data/azAZlc.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cut -f 2 data/azAZ.txt | sort -k 1 |uniq -c|sort -nr > data/azAZcc.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cut -f 1 data/azAZ.txt | sort -k 1 |uniq -c|sort -nr > data/azAZtc.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 63754169 data/azAZ.txt\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l data/azAZ.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import collections\n",
    "import math\n",
    "\n",
    "wordlist = []\n",
    "tdic = collections.defaultdict(int)\n",
    "cdic = collections.defaultdict(int)\n",
    "\n",
    "N = 63754169\n",
    "with open('data/azAZlc10.txt', 'w') as writer:\n",
    "    with open('data/azAZlc.txt') as reader:\n",
    "        for line in reader:\n",
    "            token = line.lstrip().split(' ')\n",
    "            if int(token[0]) > 9:\n",
    "                writer.write(line)\n",
    "\n",
    "with open('data/azAZtc.txt') as reader:\n",
    "    for line in reader:\n",
    "        line = line.lstrip()\n",
    "        string = re.split(\" \",line[:-1])\n",
    "        tdic[string[1]] = int(string[0])\n",
    "        \n",
    "with open('data/azAZcc.txt') as reader:\n",
    "    for line in reader:\n",
    "        line = line.lstrip()\n",
    "        string =re.split(\" \",line[:-1])  \n",
    "        cdic[string[1]]=int(string[0])\n",
    "\n",
    "with open('data/azAZ_ppmi_matrix.tsv', 'w') as writer:\n",
    "    with open('data/azAZlc10.txt') as reader:\n",
    "        for line in reader:\n",
    "            line = line.lstrip()\n",
    "            string = re.split(\" \", line[:-1])  \n",
    "            wordlist = re.split('\\t', string[1])   \n",
    "            if int(string[0]) > 0 and tdic[wordlist[0]] > 0 and cdic[wordlist[1]] > 0: \n",
    "                wtc = math.log(N) + math.log(int(string[0])) - math.log(tdic[wordlist[0]]) - math.log(cdic[wordlist[1]])\n",
    "            if wtc > 0:\n",
    "                writer.write(string[1]+'\\t'+str(wtc)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "contextdic = {}\n",
    "worddic = {}\n",
    "keylist = []\n",
    "vectorlist=[]\n",
    "\n",
    "with open('data/azAZ_ppmi_matrix.tsv') as reader:\n",
    "    for line in reader:\n",
    "        string = re.split(\"\\t\",line[:-1])\n",
    "        context = string[1]\n",
    "        value = string[2]\n",
    "        contextdic[context[:-1]] = float(value[1:])\n",
    "        if string[0] in worddic:\n",
    "            worddic[string[0]].update(contextdic)\n",
    "        else:\n",
    "            worddic[string[0]] = contextdic\n",
    "        contextdic = {}\n",
    "\n",
    "n = 0\n",
    "for k, v in worddic.items():\n",
    "    keylist.append(k)\n",
    "    vectorlist.append(v)\n",
    "    n += 1\n",
    "\n",
    "vec = DictVectorizer(sparse=True)\n",
    "array_vectors = vec.fit_transform(vectorlist)\n",
    "tsvd = TruncatedSVD(n_components=300)\n",
    "word_pca = tsvd.fit_transform(array_vectors)\n",
    "\n",
    "n = 0\n",
    "with open('data/85vec.txt', 'w') as writer:\n",
    "    while n < len(keylist):\n",
    "        writer.write(keylist[n])\n",
    "        writer.write(\" \")\n",
    "        for m in word_pca[n]:\n",
    "            precision = 6\n",
    "            m = str(np.round(m, precision))\n",
    "            writer.write(m)\n",
    "            writer.write(\" \")\n",
    "        n += 1\n",
    "        writer.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 86. 単語ベクトルの表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "United_States 1.681861 0.085434 -0.06303 -1.023177 0.231388 -0.119188 -0.742704 -0.295275 0.198685 -0.637907 1.277905 0.135483 0.160261 0.147031 0.236659 -0.133355 -0.397195 0.028645 -0.934431 0.631277 0.377157 -0.753627 -0.361601 -0.186936 -0.205268 0.432643 -0.817978 0.127752 -0.433778 0.223615 0.295197 -0.368159 -0.227965 -0.500754 -0.743579 -0.08421 -0.106091 0.461901 -0.055565 -0.410218 -0.23475 -0.642326 -0.448761 0.280719 -0.190798 -0.377314 -0.056741 0.307418 0.035419 0.625053 -0.112156 -0.353827 -0.502621 -0.730596 -0.413076 0.002332 0.247703 -0.187419 0.085918 0.072842 -0.241111 -0.574111 -0.637187 0.366203 0.126037 0.24414 0.193293 0.355025 -0.172382 0.458598 -0.228147 0.702598 0.200514 -0.126114 -0.019397 0.078634 0.277168 -0.111515 -0.186284 0.232299 -1.200607 0.148576 -0.362347 -0.227116 -0.132558 0.342389 -0.237071 -0.018675 -0.246201 -0.087795 -0.074544 -0.389725 -0.021872 -0.178654 0.368487 -0.347668 -0.028369 0.036417 0.068762 -0.035093 -0.423067 0.373216 -0.087638 -0.366049 0.947852 0.080995 0.447189 -0.079466 0.207558 -0.661116 -0.074936 -0.108317 0.33475 0.598521 -0.382681 -0.010337 -0.346248 -0.329216 0.021034 0.663331 -0.170306 -0.816928 -0.110643 0.441451 0.22597 0.197929 -0.182532 -0.610143 -0.008229 -0.278234 0.874056 0.044971 0.270586 0.692215 0.027054 0.694402 0.088716 -0.458506 -0.01715 0.079976 -0.124773 0.150508 -0.229023 0.226029 0.299546 0.059465 -0.587189 0.986779 -0.016176 0.546854 0.03111 -0.146392 0.50618 0.093039 0.644647 0.540849 -0.204756 -0.480315 -0.175841 -0.043659 -0.374715 -0.120847 -0.56052 -0.363084 -0.113933 0.546631 0.663783 0.387021 -0.2287 -0.546461 -0.613751 0.118528 -0.229353 0.389849 -0.335834 0.386166 -0.097886 0.085959 0.002533 0.369218 -0.034835 0.241736 0.029179 -0.076935 0.487159 -0.249235 -0.091993 -0.411602 -0.074643 0.295583 0.113427 -0.020217 -0.485145 -0.312761 0.043202 -0.247157 -0.527295 0.092278 0.189963 -0.247681 0.154101 0.149437 0.164315 -0.114819 0.428945 0.425501 -0.31547 0.108218 -0.250387 0.07646 -0.232226 -0.025476 0.157789 -0.205099 0.159913 0.31932 0.065586 -0.433459 0.646139 -0.002538 0.165608 -0.246054 -0.110952 -0.041215 0.311573 0.021466 -0.080677 -0.443111 0.013623 0.414709 -0.038138 0.229522 -0.025434 0.133739 0.499393 0.331148 0.117822 -0.181431 -0.368721 0.240123 0.512732 -0.355651 -0.329294 0.017298 -0.157651 0.175494 0.8822 -0.055865 0.100016 -0.140339 -0.18651 -0.495079 0.056669 -0.506488 -0.176368 0.022163 0.139898 -0.226759 0.306933 0.595776 -0.212196 0.099382 0.608773 0.287003 0.209507 -0.29528 -0.237184 0.415607 -0.441903 -0.132329 -0.347594 -0.107685 0.148832 0.239873 0.417095 0.280573 -0.027282 0.58608 0.234816 0.491468 -0.089353 0.173024 0.242203 0.524963 0.461464 -0.21929 0.688121 -0.041406 -0.106444 -0.154348 -0.097952 -0.082346 0.2501 -0.163863 -0.602342 -0.435936 -0.391289 -0.197843 -0.16371 -0.042606 \n\n"
     ]
    }
   ],
   "source": [
    "with open('data/85vec.txt') as reader:\n",
    "    for line in reader:\n",
    "        token = line.split(' ')\n",
    "        if token[0] == 'United_States':\n",
    "            print(line)\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 87. 単語の類似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.639149269983\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "a = []\n",
    "b = []\n",
    "\n",
    "with open('data/85vec.txt') as reader:\n",
    "    for line in reader: \n",
    "        string1 = re.split(\" \",line[:-1])\n",
    "        if string1[0] == \"United_States\":\n",
    "            for x in string1[1:301]:\n",
    "                a.append(float(x))\n",
    "            break\n",
    "\n",
    "with open('data/85vec.txt') as reader:\n",
    "    for line in reader: \n",
    "        string2 = re.split(\" \",line[:-1])\n",
    "        if string2[0] == \"U.S\":\n",
    "            for x in string2[1:301]:\n",
    "                b.append(float(x))\n",
    "            break\n",
    "\n",
    "npa = np.array(a)\n",
    "npb = np.array(b)\n",
    "print(cosine_similarity(npa, npb)[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 88. 類似度の高い単語10件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "England 1.0\nAustralia 0.567139629818\nJapan 0.476707225599\nAmerica 0.457064245584\nItaly 0.455954857047\nagain 0.446182569357\nEurope 0.434703218432\nGermany 0.433606678148\nIndia 0.429013038853\nChampionship 0.417617812768\n"
     ]
    }
   ],
   "source": [
    "a = []\n",
    "\n",
    "with open('data/85vec.txt') as reader:\n",
    "    for line in reader: \n",
    "        string1 = re.split(\" \",line[:-1])\n",
    "        if string1[0] == \"England\":\n",
    "            for x in string1[1:301]:\n",
    "                a.append(float(x))\n",
    "            npa = np.array(a)\n",
    "            break\n",
    "\n",
    "kv_list = []\n",
    "with open('data/85vec.txt') as reader:\n",
    "    for line in reader: \n",
    "        b = []\n",
    "        string2 = re.split(\" \",line[:-1])\n",
    "        for x in string2[1:301]:\n",
    "            b.append(float(x))\n",
    "        npb = np.array(b)\n",
    "        k = string2[0]\n",
    "        v = cosine_similarity(npa, npb)[0][0]\n",
    "        kv_list.append([k,v])\n",
    "\n",
    "count = 0    \n",
    "for k, v in sorted(kv_list, key=lambda x:x[1], reverse=True):\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 9:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 89. 加法構成性によるアナロジー"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spain 0.889071497486\nLyon 0.680407806087\nthreats 0.673486712213\nBritain 0.662073783919\ndecreased 0.657710146684\nSyria 0.651574379719\nprohibit 0.645281225816\nfarther 0.644813637681\nnorthward 0.644462093844\nItaly 0.644176907087\nGreece 0.342521964218\n"
     ]
    }
   ],
   "source": [
    "def getvec(word):\n",
    "    a = []\n",
    "    with open('data/85vec.txt') as reader:\n",
    "        for line in reader: \n",
    "            string1 = re.split(\" \",line[:-1])\n",
    "            if string1[0] == word:\n",
    "                for x in string1[1:301]:\n",
    "                    a.append(float(x))\n",
    "                npx = np.array(a)\n",
    "                break\n",
    "    return npx\n",
    "\n",
    "npa = getvec(\"Spain\")\n",
    "npb = getvec(\"Madrid\")\n",
    "npc = getvec(\"Athens\")\n",
    "npt = getvec(\"Greece\")\n",
    "npd = npa - npb + npc\n",
    " \n",
    "kv_list = []\n",
    "with open('data/85vec.txt') as reader:\n",
    "    for line in reader: \n",
    "        e = []\n",
    "        string2 = re.split(\" \",line[:-1])\n",
    "        for x in string2[1:301]:\n",
    "            e.append(float(x))\n",
    "        npe = np.array(e)\n",
    "        k = string2[0]\n",
    "        v = cosine_similarity(npd, npe)[0][0]\n",
    "        kv_list.append([k,v])\n",
    "        \n",
    "count = 0    \n",
    "for k, v in sorted(kv_list, key=lambda x:x[1], reverse=True):\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 9:\n",
    "        break\n",
    "\n",
    "print('Greece', cosine_similarity(npd, npt)[0][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}